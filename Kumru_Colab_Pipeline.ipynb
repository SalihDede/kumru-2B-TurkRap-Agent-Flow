{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kumru - Fincan Kahvem Hatrina\n",
    "## AI Rap Song Generation Pipeline (Colab Edition)\n",
    "\n",
    "Bu notebook, **Kumru-2B** base model + **LoRA adapter** (Sagopa Kajmer & Ceza) kullanarak\n",
    "tam otonom sarki uretim pipeline'ini calistirir.\n",
    "\n",
    "### Gereksinimler\n",
    "- Colab'a yuklenmus `CezaWeights/` ve `SagoWeights/` klasorleri\n",
    "- `.env` dosyasi (OpenRouter API key icin)\n",
    "- **GPU runtime** (T4 yeterli, 2B model ~4GB VRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Kontrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kutuphane Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.36.0 peft>=0.18.0 accelerate>=0.25.0 \\\n",
    "    bitsandbytes>=0.41.0 safetensors>=0.4.0 \\\n",
    "    langgraph>=0.2.0 langchain-core>=0.3.0 \\\n",
    "    httpx>=0.25.0 python-dotenv>=1.0.0 pydantic>=2.0.0 \\\n",
    "    nest_asyncio>=1.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dosyalari Yukle\n",
    "\n",
    "Colab'a su klasorleri yuklemis olmaniz gerekiyor:\n",
    "- `CezaWeights/` (adapter_config.json, adapter_model.safetensors, tokenizer dosyalari)\n",
    "- `SagoWeights/` (adapter_config.json, adapter_model.safetensors, tokenizer dosyalari)\n",
    "- `.env` dosyasi\n",
    "\n",
    "Eger Google Drive'dan yukleyecekseniz asagidaki cell'i kullanin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# YONTEM 1: Google Drive'dan yukle\n",
    "# ========================================\n",
    "# Drive'iniza CezaWeights, SagoWeights ve .env dosyasini yukleyin\n",
    "# ve asagidaki path'i guncelleyin\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Drive'daki klasor yolunu buraya yazin\n",
    "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/FincanKahvemHatrina\"  # <-- BUNU GUNCELLEYIN\n",
    "\n",
    "import shutil, os\n",
    "\n",
    "for folder in [\"CezaWeights\", \"SagoWeights\"]:\n",
    "    src = os.path.join(DRIVE_PROJECT_PATH, folder)\n",
    "    dst = f\"/content/{folder}\"\n",
    "    if os.path.exists(src) and not os.path.exists(dst):\n",
    "        shutil.copytree(src, dst)\n",
    "        print(f\"Kopyalandi: {src} -> {dst}\")\n",
    "    elif os.path.exists(dst):\n",
    "        print(f\"Zaten mevcut: {dst}\")\n",
    "    else:\n",
    "        print(f\"BULUNAMADI: {src}\")\n",
    "\n",
    "env_src = os.path.join(DRIVE_PROJECT_PATH, \".env\")\n",
    "if os.path.exists(env_src):\n",
    "    shutil.copy2(env_src, \"/content/.env\")\n",
    "    print(f\"Kopyalandi: {env_src} -> /content/.env\")\n",
    "else:\n",
    "    print(f\"BULUNAMADI: {env_src}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# YONTEM 2: Colab'a elle yukleme (upload)\n",
    "# ========================================\n",
    "# Sidebar'dan dosyalari suruklemeniz yeterli.\n",
    "# Asagidaki cell dosyalarin varligini kontrol eder.\n",
    "\n",
    "import os\n",
    "\n",
    "required = [\n",
    "    \"/content/CezaWeights/adapter_config.json\",\n",
    "    \"/content/CezaWeights/adapter_model.safetensors\",\n",
    "    \"/content/SagoWeights/adapter_config.json\",\n",
    "    \"/content/SagoWeights/adapter_model.safetensors\",\n",
    "    \"/content/.env\",\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for path in required:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"EKSIK\"\n",
    "    print(f\"  [{status}] {path}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nTum dosyalar mevcut!\")\n",
    "else:\n",
    "    print(\"\\nEKSIK dosyalar var! Lutfen yukleyin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Variables (.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"/content/.env\")\n",
    "\n",
    "# Colab icin: Lokal LoRA kullan (RunPod degil)\n",
    "os.environ[\"USE_LOCAL_MODELS\"] = \"true\"\n",
    "os.environ[\"SAGOPA_LORA_PATH\"] = \"/content/SagoWeights\"\n",
    "os.environ[\"CEZA_LORA_PATH\"] = \"/content/CezaWeights\"\n",
    "\n",
    "print(\"Environment degiskenleri:\")\n",
    "print(f\"  USE_LOCAL_MODELS  = {os.getenv('USE_LOCAL_MODELS')}\")\n",
    "print(f\"  SAGOPA_LORA_PATH  = {os.getenv('SAGOPA_LORA_PATH')}\")\n",
    "print(f\"  CEZA_LORA_PATH    = {os.getenv('CEZA_LORA_PATH')}\")\n",
    "print(f\"  OPENAI_API_KEY    = {os.getenv('OPENAI_API_KEY', '')[:20]}...\")\n",
    "print(f\"  OPENROUTHER_MODEL = {os.getenv('OPENROUTHER_MODEL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kumru-2B Base Model + LoRA Yukleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(\"kumru_colab\")\n",
    "\n",
    "BASE_MODEL_NAME = \"vngrs-ai/Kumru-2B\"\n",
    "SAGOPA_LORA_PATH = \"/content/SagoWeights\"\n",
    "CEZA_LORA_PATH = \"/content/CezaWeights\"\n",
    "\n",
    "# Base model yukle\n",
    "print(\"Base model yukleniyor: Kumru-2B ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "model_size_gb = sum(p.numel() * p.element_size() for p in base_model.parameters()) / 1024**3\n",
    "print(f\"Base model yuklendi! (~{model_size_gb:.2f} GB)\")\n",
    "print(f\"Device: {base_model.device}\")\n",
    "\n",
    "# LoRA adapter'larini yukle\n",
    "print(\"\\nSagopa LoRA yukleniyor...\")\n",
    "sagopa_model = PeftModel.from_pretrained(base_model, SAGOPA_LORA_PATH)\n",
    "print(\"Sagopa LoRA yuklendi!\")\n",
    "\n",
    "print(\"\\nCeza LoRA yukleniyor...\")\n",
    "ceza_model = PeftModel.from_pretrained(base_model, CEZA_LORA_PATH)\n",
    "print(\"Ceza LoRA yuklendi!\")\n",
    "\n",
    "print(\"\\nTum modeller hazir!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference Fonksiyonlari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_chatml_prompt(\n",
    "    instruction: str,\n",
    "    input_text: str,\n",
    "    artist: str = \"sagopa\",\n",
    "    system_prompt: str = \"\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Kumru-2B ChatML formatinda prompt olustur.\n",
    "\n",
    "    ChatML yapisi:\n",
    "        <|im_start|>system\\n{system}\\n<|im_end|>\n",
    "        <|im_start|>user\\n{instruction}\\n{input}\\n<|im_end|>\n",
    "        <|im_start|>assistant\\n\n",
    "\n",
    "    Egitim dataseti formati:\n",
    "        instruction = \"Asagidaki temada {Artist} tarzi bir rap verse'u yaz.\"\n",
    "        input       = \"{tema/konu}\"\n",
    "        output      = \"{sarki sozu}\"\n",
    "    \"\"\"\n",
    "    # Varsayilan instruction (dataset formatiyla ayni)\n",
    "    if not instruction:\n",
    "        if artist.lower() == \"sagopa\":\n",
    "            instruction = \"Asagidaki temada Sagopa Kajmer tarzi bir rap verse'u yaz.\"\n",
    "        elif artist.lower() == \"ceza\":\n",
    "            instruction = \"Asagidaki temada Ceza tarzi bir rap verse'u yaz.\"\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # System mesaji (varsa)\n",
    "    if system_prompt:\n",
    "        parts.append(f\"<|im_start|>system\\n{system_prompt}<|im_end|>\")\n",
    "\n",
    "    # User mesaji: instruction + input\n",
    "    parts.append(\n",
    "        f\"<|im_start|>user\\n\"\n",
    "        f\"{instruction}\\n\"\n",
    "        f\"{input_text}<|im_end|>\"\n",
    "    )\n",
    "\n",
    "    # Assistant baslangici (model buradan devam edecek)\n",
    "    parts.append(\"<|im_start|>assistant\\n\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def clean_special_tokens(text: str) -> str:\n",
    "    \"\"\"LoRA model output'larindan special token'lari ve copu temizler.\"\"\"\n",
    "    text = re.sub(r\"<\\|im_start\\|>.*?(?=\\n|$)\", \"\", text)\n",
    "    text = re.sub(r\"<\\|im_end\\|>\", \"\", text)\n",
    "    text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
    "\n",
    "    meta_patterns = [\n",
    "        r\"^(?:Sozler|Ask Sozleri|Ruh Hali Teknikleri|Akor Tekni[g\\u011f]i|\"\n",
    "        r\"Verse|Mood|Tema|Yonerge|Not|Review notu|Son Tahlilde|\"\n",
    "        r\"Vokal vurgusu|Downbeat|Rhyme):.*$\",\n",
    "        r\"^\\(Vokal vurgusu:.*?\\)$\",\n",
    "        r\"^\\u2b50.*$\",\n",
    "        r\"^\\u2600\\ufe0f.*$\",\n",
    "        r\"^---+$\",\n",
    "        r\"^#+\\s.*$\",\n",
    "        r\"^\\*\\*.*\\*\\*:?$\",\n",
    "        r'^_\\\".*\\\"_+_*$',\n",
    "    ]\n",
    "    for pattern in meta_patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    prev_line = None\n",
    "    repeat_count = 0\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped == prev_line and stripped:\n",
    "            repeat_count += 1\n",
    "            if repeat_count >= 2:\n",
    "                continue\n",
    "        else:\n",
    "            repeat_count = 0\n",
    "        cleaned_lines.append(line)\n",
    "        prev_line = stripped\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def generate_verse(prompt: str, artist: str = \"sagopa\",\n",
    "                   instruction: str = \"\",\n",
    "                   system_prompt: str = \"\",\n",
    "                   max_new_tokens: int = 512,\n",
    "                   temperature: float = 0.8,\n",
    "                   top_p: float = 0.9,\n",
    "                   repetition_penalty: float = 1.3) -> str:\n",
    "    \"\"\"\n",
    "    Verilen artist LoRA modeli ile verse uret.\n",
    "\n",
    "    Args:\n",
    "        prompt: Tema/konu metni (ChatML user mesajindaki input)\n",
    "        artist: 'sagopa' veya 'ceza'\n",
    "        instruction: User mesajindaki instruction (bos ise varsayilan kullanilir)\n",
    "        system_prompt: ChatML system mesaji (opsiyonel, stil profili icin)\n",
    "        max_new_tokens: Uretilecek max token\n",
    "        temperature: Sampling sicakligi\n",
    "        top_p: Nucleus sampling p\n",
    "        repetition_penalty: Tekrar cezasi\n",
    "    \"\"\"\n",
    "    model = sagopa_model if artist.lower() == \"sagopa\" else ceza_model\n",
    "\n",
    "    formatted_prompt = format_chatml_prompt(\n",
    "        instruction=instruction,\n",
    "        input_text=prompt,\n",
    "        artist=artist,\n",
    "        system_prompt=system_prompt,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "    model_inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    input_len = model_inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_ids = model_outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=False)\n",
    "    if \"<|im_end|>\" in response:\n",
    "        response = response.split(\"<|im_end|>\")[0]\n",
    "\n",
    "    return clean_special_tokens(response.strip())\n",
    "\n",
    "\n",
    "print(\"Inference fonksiyonlari hazir!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hizli Test - Tek Verse Uretimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagopa test\n",
    "print(\"=\" * 60)\n",
    "print(\"SAGOPA KAJMER TEST\")\n",
    "print(\"=\" * 60)\n",
    "sagopa_verse = generate_verse(\n",
    "    prompt=\"Hayatin anlami ve melankolik yalnizlik\",\n",
    "    artist=\"sagopa\",\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(sagopa_verse)\n",
    "\n",
    "print()\n",
    "\n",
    "# Ceza test\n",
    "print(\"=\" * 60)\n",
    "print(\"CEZA TEST\")\n",
    "print(\"=\" * 60)\n",
    "ceza_verse = generate_verse(\n",
    "    prompt=\"Sokak hayati ve mucadele\",\n",
    "    artist=\"ceza\",\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(ceza_verse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LangGraph Pipeline - Lokal LoRA Endpoint Tanimi\n",
    "\n",
    "Pipeline'in Colab'da calismasi icin `LocalLoRAEndpoint`'i tanimliyoruz.\n",
    "Bu endpoint, yukarida yukledigimiz modelleri dogrudan kullaniyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import asyncio\nimport json\nimport random\nimport nest_asyncio\nfrom datetime import datetime\nfrom typing import TypedDict, Literal\nfrom langgraph.graph import StateGraph, END\nimport httpx\nimport threading\n\nnest_asyncio.apply()\n\n# ============================================================\n# SHARED MEMORY - Paralel node'lar arasi canli iletisim\n# ============================================================\n# LangGraph paralel node'lari state'i paylasamaz (her biri kendi kopyasiyla calisir).\n# Bu global dict, sagopa ve ceza node'larinin birbirlerinin verse'lerini\n# yazim sirasinda canli olarak gormesini saglar.\n\nclass SharedMemory:\n    \"\"\"Thread-safe shared memory for live cross-artist awareness.\"\"\"\n    def __init__(self):\n        self._lock = threading.Lock()\n        self._verses = {}  # {\"sagopa_v1\": \"...\", \"ceza_v1\": \"...\", ...}\n\n    def write(self, section: str, verse: str):\n        \"\"\"Verse yazildiginda memory'ye kaydet.\"\"\"\n        with self._lock:\n            self._verses[section] = verse\n            print(f\"  [SHARED_MEMORY] {section} yazildi ({len(verse)} kar)\")\n\n    def read_other(self, artist: str) -> str:\n        \"\"\"Diger sanatcinin su ana kadar yazdigi verse'leri oku.\"\"\"\n        with self._lock:\n            parts = []\n            for section, verse in self._verses.items():\n                # Sadece diger sanatcinin verse'lerini dondur\n                if artist == \"sagopa\" and section.startswith(\"ceza_\"):\n                    parts.append(f\"[{section}]: {verse[:300]}\")\n                elif artist == \"ceza\" and section.startswith(\"sagopa_\"):\n                    parts.append(f\"[{section}]: {verse[:300]}\")\n            return \"\\n\".join(parts)\n\n    def read_all(self) -> str:\n        \"\"\"Tum verse'leri oku (critic/hook icin).\"\"\"\n        with self._lock:\n            parts = []\n            for section, verse in sorted(self._verses.items()):\n                parts.append(f\"[{section}]: {verse[:300]}\")\n            return \"\\n\".join(parts)\n\n    def clear(self, keep_sections: list = None):\n        \"\"\"Yeni iterasyon icin temizle. keep_sections verilirse sadece onlari korur.\"\"\"\n        with self._lock:\n            if keep_sections:\n                kept = {k: v for k, v in self._verses.items() if k in keep_sections}\n                removed = [k for k in self._verses if k not in keep_sections]\n                self._verses = kept\n                print(f\"  [SHARED_MEMORY] Temizlendi (korunan: {list(kept.keys())}, silinen: {removed})\")\n            else:\n                self._verses.clear()\n                print(\"  [SHARED_MEMORY] Tamamen temizlendi\")\n\nshared_memory = SharedMemory()\n\n\n# ============================================================\n# STATE DEFINITION\n# ============================================================\n\nclass SongState(TypedDict):\n    theme: str\n    mood: str\n    additional_directives: str\n    sagopa_style_prompt: str\n    ceza_style_prompt: str\n    song_structure: list[str]\n    section_briefs: dict\n    sagopa_verses: list[str]\n    ceza_verses: list[str]\n    hook: str\n    critic_scores: dict\n    critic_feedback: dict\n    revision_count: int\n    hook_revision_count: int\n    hook_critic_score: float\n    hook_critic_feedback: str\n    song_title: str\n    final_song: str\n    beat_suggestion: dict\n    status: str\n\n\n# ============================================================\n# ENDPOINT CLIENTS\n# ============================================================\n\nclass OpenAIEndpoint:\n    \"\"\"OpenAI-compatible endpoint (OpenRouter).\"\"\"\n    def __init__(self, base_url: str, model_name: str, api_key: str = \"\"):\n        self.base_url = base_url.rstrip(\"/\")\n        self.model_name = model_name\n        self.headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    async def generate(self, prompt: str, system_prompt: str = \"\", max_tokens: int = 1024) -> str:\n        payload = {\n            \"model\": self.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"max_tokens\": max_tokens,\n            \"temperature\": 0.8,\n            \"top_p\": 0.9,\n        }\n        url = f\"{self.base_url}/v1/chat/completions\"\n        print(f\"  >>> [{self.model_name}] istek -> {url}\")\n        async with httpx.AsyncClient(timeout=60.0) as client:\n            response = await client.post(url, json=payload, headers=self.headers)\n            response.raise_for_status()\n            data = response.json()\n            content = data[\"choices\"][0][\"message\"][\"content\"]\n            print(f\"  <<< [{self.model_name}] cevap ({len(content)} kar)\")\n            return content\n\n\nclass ColabLoRAEndpoint:\n    \"\"\"Colab'da yuklenmis LoRA modelleri ile dogrudan inference.\"\"\"\n    def __init__(self, artist: str):\n        self.artist = artist\n        self.model_name = f\"{artist}-lora-colab\"\n\n    async def generate(self, prompt: str, system_prompt: str = \"\", max_tokens: int = 512) -> str:\n        print(f\"  >>> [{self.model_name}] Lokal LoRA inference basliyor...\")\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None,\n            lambda: generate_verse(\n                prompt=prompt,\n                artist=self.artist,\n                instruction=system_prompt,\n                system_prompt=\"\",\n                max_new_tokens=max_tokens,\n                temperature=0.8,\n                top_p=0.9,\n                repetition_penalty=1.3,\n            )\n        )\n        print(f\"  <<< [{self.model_name}] cevap ({len(result)} kar)\")\n        return result\n\n\n# Endpoint'leri olustur\nsagopa_endpoint = ColabLoRAEndpoint(artist=\"sagopa\")\nceza_endpoint = ColabLoRAEndpoint(artist=\"ceza\")\n\norchestrator_endpoint = OpenAIEndpoint(\n    base_url=\"https://openrouter.ai/api\",\n    model_name=os.getenv(\"OPENROUTHER_MODEL\", \"anthropic/claude-haiku-4.5\"),\n    api_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n)\n\nprint(\"Endpoint'ler ve Shared Memory hazir!\")\nprint(f\"  Sagopa: ColabLoRAEndpoint (GPU uzerinde)\")\nprint(f\"  Ceza:   ColabLoRAEndpoint (GPU uzerinde)\")\nprint(f\"  Orchestrator: OpenRouter ({os.getenv('OPENROUTHER_MODEL', 'anthropic/claude-haiku-4.5')})\")\nprint(f\"  SharedMemory: Aktif (thread-safe)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pipeline Node Fonksiyonlari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# HELPER\n# ============================================================\n\ndef extract_json(text: str):\n    \"\"\"Markdown code block veya duz JSON'dan dict parse eder.\"\"\"\n    match = re.search(r\"```(?:json)?\\s*\\n?(.*?)```\", text, re.DOTALL)\n    if match:\n        try:\n            return json.loads(match.group(1).strip())\n        except json.JSONDecodeError:\n            pass\n    try:\n        return json.loads(text.strip())\n    except json.JSONDecodeError:\n        pass\n    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n    if match:\n        try:\n            return json.loads(match.group(0))\n        except json.JSONDecodeError:\n            pass\n    return None\n\n\ndef safe_score(raw_score) -> float:\n    \"\"\"Critic skorunu guvenli sekilde float'a cevir.\n    Haiku bazen 7.2 yerine 72, 6.5 yerine 65 dondurebiliyor.\n    10'dan buyuk skorlari 10'a boler.\"\"\"\n    try:\n        s = float(raw_score)\n        if s > 10:\n            s = s / 10.0\n        return round(s, 1)\n    except (TypeError, ValueError):\n        return 5.0\n\n\ndef is_verse_empty(text: str) -> bool:\n    cleaned = clean_special_tokens(text)\n    if not cleaned or len(cleaned) < 20:\n        return True\n    return False\n\n\n# ============================================================\n# SYSTEM PROMPTS\n# ============================================================\n\nIDEATION_SYSTEM = \"\"\"Sen bir sarki konsept gelistiricisisin.\nGorev: Iki sanatcinin birlikte yapacagi bir sarki icin orijinal bir konsept olustur.\n\nSADECE JSON dondur, baska hicbir sey yazma:\n{\"theme\": \"...\", \"mood\": \"...\", \"directives\": \"...\"}\n\"\"\"\n\nSTYLE_PROFILER_SYSTEM = \"\"\"Sen bir sanatci stil analisti ve profil olusturucususun.\nGorevin: Verilen sanatcilar icin, sarki sozleri yazarken kullanilacak stil profilleri olusturmak.\n\nSADECE JSON dondur:\n{\"sagopa_style_prompt\": \"...\", \"ceza_style_prompt\": \"...\"}\n\"\"\"\n\nPRODUCER_SYSTEM = \"\"\"Sen bir sarki produktorusun.\nGorevin verilen tema ve mood'a gore sarki yapisini planlamak.\nBolum isimleri icin sadece su key'leri kullan: intro, sagopa_v1, sagopa_v2, ceza_v1, ceza_v2, hook, hook_repeat, outro.\n\nSADECE JSON dondur:\n{\"structure\": [\"bolum1\", ...], \"briefs\": {\"bolum1\": \"kisa yonerge\", ...}}\n\"\"\"\n\n# Critic'e ACIK talimat: score 1-10 arasi ONDALIKLI sayi olmali\nCRITIC_SYSTEM = \"\"\"Sen bir sarki sozu elestirmenisin.\nGorevin verilen verse'i degerlendirmek. Degerlendirme kriterleri:\n1. Sanatcinin kendi stiline uygunluk (flow, kelime secimi, karakteristik ifadeler)\n2. Diger sanatcinin verse'leriyle tematik uyum ve birbirini tamamlama\n3. Lirik kalite, metafor kullanimi, akicilik\n4. Sarkinin genel temasina ve mood'una baglilik\n\nOnemli: Iki sanatcinin verse'leri bir butun olarak uyumlu olmali. Biri digerini tamamlamali.\nBu sanatcilar kucuk bir dil modeliyle (2B parametre) uretilmis. Mukemmeli bekleme ama temel kaliteyi ara.\n\nSADECE JSON dondur. score 1 ile 10 arasi bir ONDALIKLI sayi olmali (ornek: 6.5, 7.0, 8.2).\nfeedback kisaca ve net olmali (1-2 cumle, Turkce).\n{\"score\": 7.0, \"feedback\": \"...\", \"approved\": true}\n\"\"\"\n\nHOOK_CRITIC_SYSTEM = \"\"\"Sen bir nakarat elestirmenisin.\nGorevin verilen nakarati sarkinin geneli ile uyum acisindan degerlendirmek.\nDegerlendirme kriterleri:\n1. Verse'lerle tematik uyum - nakarat sarkinin verse'leriyle butunluk saglamali\n2. Akilda kalicilik - tekrar dinlenebilir, melodik bir yapi\n3. Iki sanatcinin stilini birlestirme basarisi\n4. Sarkinin mood'una uygunluk\n\nSADECE JSON dondur. score 1 ile 10 arasi bir ONDALIKLI sayi olmali (ornek: 6.5, 7.0, 8.2).\n{\"score\": 7.0, \"feedback\": \"...\", \"approved\": true}\n\"\"\"\n\nTITLE_SYSTEM = \"\"\"Sen bir sarki isimlendirme uzmanisin.\nVerilen sarki sozlerine bakarak sarkiya uygun, akilda kalici bir isim sec.\nSadece ismi dondur, baska hicbir sey yazma.\n\"\"\"\n\n# Esik degeri: 2B model icin 6.5 daha gercekci\nVERSE_PASS_THRESHOLD = 6.5\nHOOK_PASS_THRESHOLD = 6.5\n\n\n# ============================================================\n# NODE FUNCTIONS\n# ============================================================\n\nasync def ideation_node(state: SongState) -> dict:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ADIM 0 - IDEATION: Tema/mood/directive uretiliyor...\")\n    print(\"=\" * 60)\n    seed = random.randint(1, 10000)\n    prompt = f\"Iki sanatci icin orijinal bir sarki konsepti olustur. Yaratici ol, klise temalardan kacin. (seed: {seed})\"\n    response = await orchestrator_endpoint.generate(prompt, IDEATION_SYSTEM)\n    idea = extract_json(response)\n    if not idea:\n        response = await orchestrator_endpoint.generate(prompt, IDEATION_SYSTEM)\n        idea = extract_json(response)\n        if not idea:\n            raise RuntimeError(\"Ideation JSON uretemedi.\")\n    print(f\"  Tema: {idea.get('theme', '')}\")\n    print(f\"  Mood: {idea.get('mood', '')}\")\n    print(f\"  Direktifler: {idea.get('directives', '')}\")\n    return {\"theme\": idea[\"theme\"], \"mood\": idea[\"mood\"], \"additional_directives\": idea.get(\"directives\", \"\")}\n\n\nasync def style_profiler_node(state: SongState) -> dict:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ADIM 0.5 - STYLE PROFILER\")\n    print(\"=\" * 60)\n    prompt = f\"Sarki temasi: {state['theme']}\\nMood: {state['mood']}\\nEk yonergeler: {state.get('additional_directives', '')}\\n\\nSagopa Kajmer ve Ceza icin stil profilleri olustur.\"\n    response = await orchestrator_endpoint.generate(prompt, STYLE_PROFILER_SYSTEM)\n    profiles = extract_json(response)\n    if not profiles:\n        response = await orchestrator_endpoint.generate(prompt, STYLE_PROFILER_SYSTEM)\n        profiles = extract_json(response)\n        if not profiles:\n            raise RuntimeError(\"Style profiler JSON uretemedi.\")\n    print(f\"  Sagopa prompt: {profiles.get('sagopa_style_prompt', '')[:100]}...\")\n    print(f\"  Ceza prompt:   {profiles.get('ceza_style_prompt', '')[:100]}...\")\n    return {\"sagopa_style_prompt\": profiles[\"sagopa_style_prompt\"], \"ceza_style_prompt\": profiles[\"ceza_style_prompt\"]}\n\n\nasync def producer_node(state: SongState) -> dict:\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"ADIM 1 - PRODUCER: Sarki yapisi planlaniyor... (Tema: {state['theme']})\")\n    print(\"=\" * 60)\n    prompt = f\"Tema: {state['theme']}\\nMood: {state['mood']}\\nEk Yonergeler: {state.get('additional_directives', 'Yok')}\\n\\nSarki yapisini planla.\"\n    response = await orchestrator_endpoint.generate(prompt, PRODUCER_SYSTEM, max_tokens=2048)\n    plan = extract_json(response)\n    if not plan:\n        response = await orchestrator_endpoint.generate(prompt, PRODUCER_SYSTEM, max_tokens=2048)\n        plan = extract_json(response)\n        if not plan:\n            raise RuntimeError(\"Producer JSON uretemedi.\")\n    structure = plan.get(\"structure\", [])\n    briefs = plan.get(\"briefs\", {})\n    print(f\"  Yapi: {' -> '.join(structure)}\")\n    for sec, brief in briefs.items():\n        print(f\"    [{sec}] {brief}\")\n    shared_memory.clear()\n    return {\"song_structure\": structure, \"section_briefs\": briefs, \"status\": \"planning_complete\"}\n\n\nasync def sagopa_node(state: SongState) -> dict:\n    \"\"\"Sagopa verse'lerini yazar. Shared memory uzerinden Ceza'yi canli takip eder.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"ADIM 2a - SAGOPA KAJMER: Verse yaziliyor... (iterasyon {state.get('revision_count', 0)})\")\n    print(\"=\" * 60)\n    briefs = state.get(\"section_briefs\", {})\n    verses = []\n\n    for section in state.get(\"song_structure\", []):\n        if section.startswith(\"sagopa_\"):\n            brief = briefs.get(section, state[\"theme\"])\n            feedback = state.get(\"critic_feedback\", {}).get(section, \"\")\n\n            # --- SHARED MEMORY: Ceza'nin su ana kadar yazdiklarina bak ---\n            ceza_context = shared_memory.read_other(\"sagopa\")\n\n            # Prompt'u KISA tut - 2B model uzun prompt'larla bozuluyor\n            theme_input = f\"{state['theme']}\\n{brief}\"\n\n            if ceza_context:\n                theme_input += f\"\\n\\nCeza'nin verse'leri:\\n{ceza_context}\"\n                print(f\"  [{section}] SHARED MEMORY'den Ceza context okudu\")\n\n            if feedback:\n                theme_input += f\"\\n\\n(Not: {feedback[:100]})\"\n\n            instruction = \"Asagidaki temada Sagopa Kajmer tarzi bir rap verse'u yaz.\"\n            print(f\"  [{section}] icin uretiliyor...\")\n            raw_verse = await sagopa_endpoint.generate(theme_input, instruction, max_tokens=512)\n            verse = clean_special_tokens(raw_verse) if isinstance(raw_verse, str) else raw_verse\n            if is_verse_empty(verse):\n                raw_verse = await sagopa_endpoint.generate(theme_input, instruction, max_tokens=512)\n                verse = clean_special_tokens(raw_verse)\n                if is_verse_empty(verse):\n                    verse = f\"[{section} - verse uretilemedi]\"\n            print(f\"  [{section}] Sagopa verse:\\n{verse[:200]}...\")\n\n            shared_memory.write(section, verse)\n            verses.append(verse)\n    return {\"sagopa_verses\": verses}\n\n\nasync def ceza_node(state: SongState) -> dict:\n    \"\"\"Ceza verse'lerini yazar. Shared memory uzerinden Sagopa'yi canli takip eder.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"ADIM 2b - CEZA: Verse yaziliyor... (iterasyon {state.get('revision_count', 0)})\")\n    print(\"=\" * 60)\n    briefs = state.get(\"section_briefs\", {})\n    verses = []\n\n    for section in state.get(\"song_structure\", []):\n        if section.startswith(\"ceza_\"):\n            brief = briefs.get(section, state[\"theme\"])\n            feedback = state.get(\"critic_feedback\", {}).get(section, \"\")\n\n            # --- SHARED MEMORY: Sagopa'nin su ana kadar yazdiklarina bak ---\n            sagopa_context = shared_memory.read_other(\"ceza\")\n\n            # Prompt'u KISA tut\n            theme_input = f\"{state['theme']}\\n{brief}\"\n\n            if sagopa_context:\n                theme_input += f\"\\n\\nSagopa'nin verse'leri:\\n{sagopa_context}\"\n                print(f\"  [{section}] SHARED MEMORY'den Sagopa context okudu\")\n\n            if feedback:\n                theme_input += f\"\\n\\n(Not: {feedback[:100]})\"\n\n            instruction = \"Asagidaki temada Ceza tarzi bir rap verse'u yaz.\"\n            print(f\"  [{section}] icin uretiliyor...\")\n            raw_verse = await ceza_endpoint.generate(theme_input, instruction, max_tokens=512)\n            verse = clean_special_tokens(raw_verse) if isinstance(raw_verse, str) else raw_verse\n            if is_verse_empty(verse):\n                raw_verse = await ceza_endpoint.generate(theme_input, instruction, max_tokens=512)\n                verse = clean_special_tokens(raw_verse)\n                if is_verse_empty(verse):\n                    verse = f\"[{section} - verse uretilemedi]\"\n            print(f\"  [{section}] Ceza verse:\\n{verse[:200]}...\")\n\n            shared_memory.write(section, verse)\n            verses.append(verse)\n    return {\"ceza_verses\": verses}\n\n\nasync def hook_node(state: SongState) -> dict:\n    \"\"\"Nakarat yazar. Verse'ler onaylandiktan sonra calisir.\"\"\"\n    hook_rev = state.get(\"hook_revision_count\", 0)\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"ADIM 3 - HOOK: Nakarat yaziliyor... (hook iterasyon {hook_rev})\")\n    print(\"=\" * 60)\n\n    prev_feedback = state.get(\"hook_critic_feedback\", \"\")\n\n    prompt = f\"\"\"Tema: {state['theme']}\nMood: {state['mood']}\n\nSagopa'nin verse'i (ozet): {state['sagopa_verses'][0][:200] if state.get('sagopa_verses') else 'Henuz yok'}\nCeza'nin verse'i (ozet): {state['ceza_verses'][0][:200] if state.get('ceza_verses') else 'Henuz yok'}\n\nHer iki sanatcinin tarzini birlestiren, akilda kalici bir nakarat yaz. 4-6 satir.\"\"\"\n\n    if prev_feedback:\n        prompt += f\"\\n\\n(Onceki nakarat reddedildi. Elestiri: {prev_feedback})\"\n\n    hook = await orchestrator_endpoint.generate(prompt,\n        \"Sen bir nakarat yazarisin. Verilen verse'lere ve temaya uygun nakarat yaz. Sadece nakarat satirlarini dondur.\")\n    print(f\"  Hook:\\n{hook}\")\n    return {\"hook\": hook}\n\n\nasync def critic_node(state: SongState) -> dict:\n    \"\"\"Verse'leri degerlendirir.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"ADIM 4 - VERSE CRITIC: Verse'ler degerlendiriliyor... (iterasyon {state.get('revision_count', 0) + 1})\")\n    print(\"=\" * 60)\n\n    all_sagopa = {}\n    for i, verse in enumerate(state.get(\"sagopa_verses\", [])):\n        all_sagopa[f\"sagopa_v{i+1}\"] = verse\n    all_ceza = {}\n    for i, verse in enumerate(state.get(\"ceza_verses\", [])):\n        all_ceza[f\"ceza_v{i+1}\"] = verse\n\n    all_verses = {**all_sagopa, **all_ceza}\n\n    sagopa_context_text = \"\\n\\n\".join([f\"[{k}]: {v}\" for k, v in all_sagopa.items()])\n    ceza_context_text = \"\\n\\n\".join([f\"[{k}]: {v}\" for k, v in all_ceza.items()])\n\n    scores = {}\n    feedback = {}\n    for section_name, verse_text in all_verses.items():\n        is_sagopa = \"sagopa\" in section_name\n        artist_name = \"Sagopa Kajmer\" if is_sagopa else \"Ceza\"\n        other_artist = \"Ceza\" if is_sagopa else \"Sagopa Kajmer\"\n        other_context = ceza_context_text if is_sagopa else sagopa_context_text\n\n        prompt = (\n            f\"Sanatci: {artist_name}\\n\"\n            f\"Bolum: {section_name}\\n\"\n            f\"Tema: {state['theme']}\\n\"\n            f\"Mood: {state['mood']}\\n\\n\"\n            f\"Verse:\\n{verse_text}\\n\\n\"\n            f\"--- {other_artist}'nin verse'leri (uyum icin degerlendir) ---\\n{other_context}\\n\\n\"\n            f\"Bu verse'i degerlendir.\"\n        )\n        print(f\"  [{section_name}] degerlendiriliyor...\")\n        response = await orchestrator_endpoint.generate(prompt, CRITIC_SYSTEM)\n        evaluation = extract_json(response)\n        if evaluation:\n            raw = evaluation.get(\"score\", 5)\n            score = safe_score(raw)\n            scores[section_name] = score\n            if score < VERSE_PASS_THRESHOLD:\n                feedback[section_name] = evaluation.get(\"feedback\", \"\")\n                print(f\"  [{section_name}] REDDEDILDI (skor: {score}, ham: {raw})\")\n            else:\n                print(f\"  [{section_name}] ONAYLANDI (skor: {score}, ham: {raw})\")\n        else:\n            scores[section_name] = 5.0\n            feedback[section_name] = \"Degerlendirme parse edilemedi.\"\n\n    return {\"critic_scores\": scores, \"critic_feedback\": feedback, \"revision_count\": state.get(\"revision_count\", 0) + 1}\n\n\nasync def hook_critic_node(state: SongState) -> dict:\n    \"\"\"Nakarati sarkinin geneli ile uyum acisindan degerlendirir.\"\"\"\n    hook_rev = state.get(\"hook_revision_count\", 0)\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"ADIM 4b - HOOK CRITIC: Nakarat degerlendiriliyor... (hook iterasyon {hook_rev + 1})\")\n    print(\"=\" * 60)\n\n    hook_text = state.get(\"hook\", \"\")\n    all_verses_text = shared_memory.read_all()\n\n    prompt = (\n        f\"Tema: {state['theme']}\\n\"\n        f\"Mood: {state['mood']}\\n\\n\"\n        f\"NAKARAT:\\n{hook_text}\\n\\n\"\n        f\"--- SARKININ VERSE'LERI ---\\n{all_verses_text}\\n\\n\"\n        f\"Bu nakarati degerlendir.\"\n    )\n    print(f\"  [hook] degerlendiriliyor...\")\n    response = await orchestrator_endpoint.generate(prompt, HOOK_CRITIC_SYSTEM)\n    evaluation = extract_json(response)\n\n    score = 5.0\n    fb = \"\"\n    if evaluation:\n        raw = evaluation.get(\"score\", 5)\n        score = safe_score(raw)\n        if score < HOOK_PASS_THRESHOLD:\n            fb = evaluation.get(\"feedback\", \"\")\n            print(f\"  [hook] REDDEDILDI (skor: {score}, ham: {raw})\")\n        else:\n            print(f\"  [hook] ONAYLANDI (skor: {score}, ham: {raw})\")\n    else:\n        fb = \"Degerlendirme parse edilemedi.\"\n\n    return {\n        \"hook_critic_score\": score,\n        \"hook_critic_feedback\": fb,\n        \"hook_revision_count\": hook_rev + 1,\n    }\n\n\ndef should_revise(state: SongState) -> list:\n    \"\"\"Verse revizyon karari. Ikisi de yeniden yazar ama iyi verse'ler shared memory'de kalir.\"\"\"\n    scores = state.get(\"critic_scores\", {})\n    revision_count = state.get(\"revision_count\", 0)\n\n    if revision_count >= 10:\n        print(f\"  KARAR: Guvenlik siniri (10 iterasyon) ulasildi -> HOOK YAZIMI\")\n        return [\"hook_writer\"]\n\n    if any(score < VERSE_PASS_THRESHOLD for score in scores.values()):\n        low = {k: v for k, v in scores.items() if v < VERSE_PASS_THRESHOLD}\n        high = [k for k, v in scores.items() if v >= VERSE_PASS_THRESHOLD]\n        print(f\"  KARAR: Dusuk skorlar {low} -> IKISI DE REVIZYON (iterasyon {revision_count})\")\n        # Iyi verse'leri shared memory'de koru - yeniden yazarken context olarak kullanilacak\n        shared_memory.clear(keep_sections=high if high else None)\n        return [\"sagopa_writer\", \"ceza_writer\"]\n\n    print(f\"  KARAR: Tum verse skorlar >= {VERSE_PASS_THRESHOLD} -> HOOK YAZIMI (toplam {revision_count} iterasyon)\")\n    return [\"hook_writer\"]\n\n\ndef should_revise_hook(state: SongState) -> str:\n    \"\"\"Hook revizyon karari.\"\"\"\n    score = state.get(\"hook_critic_score\", 0)\n    hook_rev = state.get(\"hook_revision_count\", 0)\n\n    if hook_rev >= 5:\n        print(f\"  KARAR: Hook guvenlik siniri (5 iterasyon) -> ASSEMBLY\")\n        return \"assembly\"\n\n    if score < HOOK_PASS_THRESHOLD:\n        print(f\"  KARAR: Hook skor {score} < {HOOK_PASS_THRESHOLD} -> HOOK TEKRAR YAZILACAK (iterasyon {hook_rev})\")\n        return \"hook_writer\"\n\n    print(f\"  KARAR: Hook skor {score} >= {HOOK_PASS_THRESHOLD} -> ASSEMBLY (toplam {hook_rev} hook iterasyonu)\")\n    return \"assembly\"\n\n\nasync def assembly_node(state: SongState) -> dict:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ADIM 5 - ASSEMBLY: Sarki birlestiriliyor...\")\n    print(\"=\" * 60)\n    structure = state.get(\"song_structure\", [])\n    sagopa_idx = 0\n    ceza_idx = 0\n    song_parts = []\n    for section in structure:\n        if section == \"intro\":\n            song_parts.append(\"[Intro]\\n...\")\n        elif section.startswith(\"sagopa_\"):\n            if sagopa_idx < len(state.get(\"sagopa_verses\", [])):\n                song_parts.append(f\"[Sagopa Kajmer - Verse {sagopa_idx + 1}]\\n{state['sagopa_verses'][sagopa_idx]}\")\n                sagopa_idx += 1\n        elif section.startswith(\"ceza_\"):\n            if ceza_idx < len(state.get(\"ceza_verses\", [])):\n                song_parts.append(f\"[Ceza - Verse {ceza_idx + 1}]\\n{state['ceza_verses'][ceza_idx]}\")\n                ceza_idx += 1\n        elif \"hook\" in section:\n            song_parts.append(f\"[Nakarat]\\n{state.get('hook', '')}\")\n        elif section == \"outro\":\n            song_parts.append(\"[Outro]\\n...\")\n    final_song_body = \"\\n\\n\".join(song_parts)\n\n    title_prompt = f\"Tema: {state['theme']}\\nMood: {state['mood']}\\n\\nSarki sozleri:\\n{final_song_body[:500]}\\n\\nBu sarkiya uygun bir isim sec.\"\n    song_title = (await orchestrator_endpoint.generate(title_prompt, TITLE_SYSTEM)).strip().strip('\"').strip(\"'\")\n    print(f\"  Sarki ismi: {song_title}\")\n\n    md_song = f\"\"\"# {song_title}\n**Sanatcilar:** Sagopa Kajmer ft. Ceza\n**Tema:** {state['theme']}\n**Mood:** {state['mood']}\n**Toplam verse revizyon:** {state.get('revision_count', 0)}\n**Toplam hook revizyon:** {state.get('hook_revision_count', 0)}\n\n{final_song_body}\n\"\"\"\n    return {\"song_title\": song_title, \"final_song\": md_song, \"status\": \"assembly_complete\"}\n\n\nasync def beat_suggestion_node(state: SongState) -> dict:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ADIM 6 - BEAT SUGGESTION\")\n    print(\"=\" * 60)\n    prompt = f\"Sarki temasi: {state['theme']}\\nMood: {state['mood']}\\n\\nBu sarki icin genel bir beat onerisi olustur. SADECE JSON dondur.\"\n    beat_sys = \"Sen bir beat maker'sin. Kisa ve oz bir JSON dondur. Genel bir oneri ver.\"\n    response = await orchestrator_endpoint.generate(prompt, beat_sys, max_tokens=512)\n    suggestion = extract_json(response)\n    if not suggestion:\n        response = await orchestrator_endpoint.generate(prompt, beat_sys, max_tokens=512)\n        suggestion = extract_json(response)\n        if not suggestion:\n            suggestion = {\"bpm\": 90, \"key\": \"Am\", \"style\": \"boom bap\"}\n    print(f\"  Beat: {json.dumps(suggestion, ensure_ascii=False, indent=2)}\")\n    return {\"beat_suggestion\": suggestion, \"status\": \"completed\"}\n\n\nprint(\"Node fonksiyonlari hazir!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Graph Olustur ve Calistir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_song_graph():\n    \"\"\"LangGraph pipeline'ini olusturur.\n\n    Akis:\n        ideation -> style_profiler -> producer\n        -> [sagopa_writer, ceza_writer] (paralel, shared memory ile canli iletisim)\n        -> verse_critic\n            -> (herhangi biri < 7) -> [sagopa_writer, ceza_writer] (ikisi de yeniden yazar)\n            -> (hepsi >= 7) -> hook_writer -> hook_critic\n                -> (hook < 7) -> hook_writer (tekrar yaz)\n                -> (hook >= 7) -> assembly -> beat_suggester -> END\n    \"\"\"\n    graph = StateGraph(SongState)\n\n    graph.add_node(\"ideation\", ideation_node)\n    graph.add_node(\"style_profiler\", style_profiler_node)\n    graph.add_node(\"producer\", producer_node)\n    graph.add_node(\"sagopa_writer\", sagopa_node)\n    graph.add_node(\"ceza_writer\", ceza_node)\n    graph.add_node(\"hook_writer\", hook_node)\n    graph.add_node(\"verse_critic\", critic_node)\n    graph.add_node(\"hook_critic\", hook_critic_node)\n    graph.add_node(\"assembly\", assembly_node)\n    graph.add_node(\"beat_suggester\", beat_suggestion_node)\n\n    # Ideation -> Style -> Producer\n    graph.set_entry_point(\"ideation\")\n    graph.add_edge(\"ideation\", \"style_profiler\")\n    graph.add_edge(\"style_profiler\", \"producer\")\n\n    # Producer -> paralel yazim (fan-out, shared memory ile birbirini goruyor)\n    graph.add_edge(\"producer\", \"sagopa_writer\")\n    graph.add_edge(\"producer\", \"ceza_writer\")\n\n    # Yazarlar -> Verse Critic (fan-in)\n    graph.add_edge(\"sagopa_writer\", \"verse_critic\")\n    graph.add_edge(\"ceza_writer\", \"verse_critic\")\n\n    # Verse Critic -> conditional:\n    #   Revizyon: ikisi de yeniden yazar (paralel fan-out)\n    #   Onay: hook yazimina gec\n    graph.add_conditional_edges(\"verse_critic\", should_revise, {\n        \"sagopa_writer\": \"sagopa_writer\",\n        \"ceza_writer\": \"ceza_writer\",\n        \"hook_writer\": \"hook_writer\",\n    })\n\n    # Hook -> Hook Critic\n    graph.add_edge(\"hook_writer\", \"hook_critic\")\n\n    # Hook Critic -> conditional:\n    #   Revizyon: hook tekrar yazilir\n    #   Onay: assembly'ye gec\n    graph.add_conditional_edges(\"hook_critic\", should_revise_hook, {\n        \"hook_writer\": \"hook_writer\",\n        \"assembly\": \"assembly\",\n    })\n\n    # Assembly -> Beat -> END\n    graph.add_edge(\"assembly\", \"beat_suggester\")\n    graph.add_edge(\"beat_suggester\", END)\n\n    return graph.compile()\n\nprint(\"Graph builder hazir!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# PIPELINE CALISTIR\n# ============================================================\n\nasync def create_song() -> dict:\n    \"\"\"Tam otonom sarki uretim pipeline'i.\"\"\"\n    app = build_song_graph()\n\n    initial_state: SongState = {\n        \"theme\": \"\",\n        \"mood\": \"\",\n        \"additional_directives\": \"\",\n        \"sagopa_style_prompt\": \"\",\n        \"ceza_style_prompt\": \"\",\n        \"song_structure\": [],\n        \"section_briefs\": {},\n        \"sagopa_verses\": [],\n        \"ceza_verses\": [],\n        \"hook\": \"\",\n        \"critic_scores\": {},\n        \"critic_feedback\": {},\n        \"revision_count\": 0,\n        \"hook_revision_count\": 0,\n        \"hook_critic_score\": 0.0,\n        \"hook_critic_feedback\": \"\",\n        \"song_title\": \"\",\n        \"final_song\": \"\",\n        \"beat_suggestion\": {},\n        \"status\": \"initialized\",\n    }\n\n    # Shared memory'yi temizle (yeni sarki)\n    shared_memory.clear()\n\n    print(\"=\" * 60)\n    print(\"PIPELINE BASLATIILIYOR (tam otonom mod)\")\n    print(\"Kumru-2B + LoRA (Colab GPU) + OpenRouter Orchestrator\")\n    print(\"Shared Memory: AKTIF (paralel yazim sirasinda canli iletisim)\")\n    print(f\"Verse esigi: {VERSE_PASS_THRESHOLD} | Hook esigi: {HOOK_PASS_THRESHOLD}\")\n    print(\"=\" * 60)\n\n    final_state = None\n    async for event in app.astream(initial_state):\n        for node_name, node_output in event.items():\n            status = node_output.get(\"status\", \"\")\n            print(f\"\\n[{node_name}] tamamlandi.{f' Status: {status}' if status else ''}\")\n            if node_name == \"verse_critic\":\n                scores = node_output.get(\"critic_scores\", {})\n                for section, score in scores.items():\n                    mark = \"PASS\" if score >= VERSE_PASS_THRESHOLD else \"FAIL\"\n                    print(f\"  {mark} {section}: {score}/10\")\n            elif node_name == \"hook_critic\":\n                score = node_output.get(\"hook_critic_score\", 0)\n                mark = \"PASS\" if score >= HOOK_PASS_THRESHOLD else \"FAIL\"\n                print(f\"  {mark} hook: {score}/10\")\n            final_state = {**initial_state, **node_output} if final_state is None else {**final_state, **node_output}\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SARKI TAMAMLANDI!\")\n    print(\"=\" * 60)\n    return final_state or {}\n\n# Calistir!\nresult = asyncio.run(create_song())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sonuclari Goruntule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "if result and result.get(\"final_song\"):\n",
    "    display(Markdown(result[\"final_song\"]))\n",
    "else:\n",
    "    print(\"Sarki uretilemedi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beat onerisi\n",
    "if result and result.get(\"beat_suggestion\"):\n",
    "    print(\"Beat Onerisi:\")\n",
    "    print(json.dumps(result[\"beat_suggestion\"], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Critic skorlari\nif result:\n    if result.get(\"critic_scores\"):\n        print(\"Verse Critic Skorlari:\")\n        for section, score in result[\"critic_scores\"].items():\n            mark = \"PASS\" if score >= VERSE_PASS_THRESHOLD else \"FAIL\"\n            print(f\"  [{mark}] {section}: {score}/10\")\n        print(f\"\\nToplam verse revizyon sayisi: {result.get('revision_count', 0)}\")\n\n    hook_score = result.get(\"hook_critic_score\", 0)\n    if hook_score:\n        mark = \"PASS\" if hook_score >= HOOK_PASS_THRESHOLD else \"FAIL\"\n        print(f\"\\nHook Critic Skoru:\")\n        print(f\"  [{mark}] hook: {hook_score}/10\")\n        print(f\"Toplam hook revizyon sayisi: {result.get('hook_revision_count', 0)}\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}